{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deterministic Model",
   "id": "6ed2991a7b93a6ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "bd6c0fbe3c95e690"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "\n",
    "d_of_x = 1\n",
    "N_rec = 40\n",
    "N_mem = 1 # memory length\n",
    "d_input = N_mem * d_of_x # number of input nodes\n",
    "d_output = d_of_x # number of output nodes\n",
    "d_outputRNN = N_rec * d_output # total nubmer of output nodes (including recurrence)\n",
    "\n",
    "n_hidden = 3 # number of hidden layers\n",
    "n_nodes = 20 # number of nodes per hidden layer\n",
    "n_epochs = 100_000 #50_000\n",
    "learning_rate = 1e-4\n",
    "N_seeds = 1 # number of seeds for ensemble learning\n",
    "batch_size = 256 # batch size\n",
    "verbose = False\n",
    "\n",
    "system = 'stochastic'\n",
    "model_upper_dir = f'./models/final{system}_{n_epochs}epochs/'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ],
   "id": "60e6423a57553862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# stochastic data\n",
    "data = np.load(\"./Data/training_data.npz\")\n",
    "\n",
    "inputs_train = data[\"inputs_train\"]\n",
    "outputs_train = data[\"outputs_train\"]\n",
    "full_train = data[\"full_train\"]\n",
    "S = data[\"S_full\"]\n",
    "t_grid = data[\"t_grid_all\"]\n",
    "\n",
    "inputs_train_torch = torch.from_numpy(inputs_train).float()\n",
    "outputs_train_torch = torch.from_numpy(outputs_train).float()\n",
    "full_train_torch = torch.from_numpy(full_train).float()\n",
    "\n",
    "inputs_train_torch = inputs_train_torch.to(device)\n",
    "outputs_train_torch = outputs_train_torch.to(device)\n",
    "full_train_torch = full_train_torch.to(device)"
   ],
   "id": "b0180131ca5d1532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deterministic Model and Training\n",
    "\n"
   ],
   "id": "112ac49f23f8af92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DeterministicSubMap(nn.Module):\n",
    "    def __init__(self, d_of_x = 1, N_mem = 1, N_rec = 40, n_hidden = 3, n_nodes = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.recurrent_steps = N_rec\n",
    "        input_dim = N_mem * d_of_x\n",
    "        output_dim = d_of_x\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(input_dim, n_nodes))        # first layer (input dim -> n_nodes)\n",
    "        for _ in range(n_hidden - 1):\n",
    "            self.hidden_layers.append(nn.Linear(n_nodes, n_nodes))      # remaining layers\n",
    "        self.last_linear = nn.Linear(n_nodes, output_dim)               # last layer\n",
    "\n",
    "        self.slice_last_two = lambda x: x[:, -d_of_x:]\n",
    "        self.slice_layer_Two = lambda x: x[:, d_of_x:]\n",
    "\n",
    "    # a single forward without skip connection added to residual\n",
    "    def forward_single(self, inputs):\n",
    "        residual = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            residual = self.ReLU(layer(residual))\n",
    "        residual = self.last_linear(residual)\n",
    "        return residual\n",
    "\n",
    "    # forward with n_reccurence = 1 for prediction\n",
    "    def forward(self, inputs):\n",
    "        x_last = self.slice_last_two(inputs)\n",
    "        residual_output = self.forward_single(inputs)\n",
    "        x_add_res = residual_output + x_last\n",
    "        return x_add_res\n",
    "\n",
    "    # forward with reccurence for training\n",
    "    def forward_recurrence(self, inputs):\n",
    "        # collect outputs at each recurrent step\n",
    "        outputs = []  # z = []\n",
    "\n",
    "        x = inputs\n",
    "        x_minus_first_two = self.slice_layer_Two(x)\n",
    "        x_last_two = self.slice_last_two(x)\n",
    "\n",
    "        residual = self.forward_single(x)\n",
    "        x_add_res = residual + x_last_two\n",
    "\n",
    "        outputs.append(x_add_res)\n",
    "\n",
    "        for _ in range(self.recurrent_steps - 1):\n",
    "            x = torch.cat([x_minus_first_two, x_add_res], dim=1)\n",
    "            x_minus_first_two = self.slice_layer_Two(x)\n",
    "            x_last_two = self.slice_last_two(x)\n",
    "\n",
    "            residual = self.forward_single(x)\n",
    "            x_add_res = residual + x_last_two\n",
    "\n",
    "            outputs.append(x_add_res)\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "id": "836bf2da35a6e42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for seed in range(N_seeds):\n",
    "    print(f\"training model {seed + 1}\")\n",
    "\n",
    "    model_dir = model_upper_dir + f'model_seed_{seed}/'\n",
    "    model_weights_path = os.path.join(model_dir, 'best_weights.pth')\n",
    "    plot_path = os.path.join(model_dir, 'loss_semilog.png')\n",
    "    # print(\"model_dir: \", model_dir)\n",
    "\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    train_dataset = TensorDataset(inputs_train_torch, outputs_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = DeterministicSubMap(d_of_x = d_of_x, N_mem = N_mem, N_rec = N_rec, n_hidden = n_hidden, n_nodes = n_nodes)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    history_loss = []\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model.forward_recurrence(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = running_loss / len(train_loader)\n",
    "        history_loss.append(avg_epoch_loss)\n",
    "\n",
    "        # Keras ModelCheckpoint (monitor=\"loss\", save_best_only=True)\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            # Save ONLY best weights\n",
    "            torch.save(model.state_dict(), model_weights_path)\n",
    "            saved_best = True\n",
    "        else:\n",
    "            saved_best = False\n",
    "\n",
    "        if verbose:\n",
    "            if (epoch + 1) % 10 == 0 or epoch == n_epochs - 1:\n",
    "                print(f'Epoch {epoch + 1:4d}/{n_epochs}, Loss: {avg_epoch_loss:.6f} {\"(Saved Best)\" if saved_best else \"\"}')\n",
    "        if not verbose and (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1:5d} completed\")\n",
    "\n",
    "    plt.semilogy(history_loss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.savefig(model_dir + 'loss_semilog.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print()"
   ],
   "id": "c8091b9f72931dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stochastic GAN",
   "id": "9358fac3564b8f79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "6940732331cccdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "L = N_rec\n",
    "# x at t0\n",
    "x0_all = inputs_train_torch              # (N, 1)\n",
    "# x_t1 through x_tL\n",
    "x_all_1_to_L = outputs_train_torch        # (N, L)\n",
    "# x_0 to x_{L-1}\n",
    "x_all_0_to_Lminus1 = full_train_torch[:, :-1]      # (N, L)\n",
    "# Increments y_n = x_n - x_{n-1}, for all n = 1 to L\n",
    "y_all = x_all_1_to_L - x_all_0_to_Lminus1          # (N, L)\n",
    "# GAN training data: concatenated [x0, y1..yL]\n",
    "gan_sequences = torch.cat([x0_all, y_all], dim=1)  # shape (N, 1 + L)\n",
    "\n",
    "gan_dataset = TensorDataset(gan_sequences.to(device))\n",
    "gan_loader  = DataLoader(gan_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "det_model = DeterministicSubMap(d_of_x=d_of_x, N_mem=N_mem, N_rec=N_rec,\n",
    "                                n_hidden=n_hidden, n_nodes=n_nodes).to(device)\n",
    "\n",
    "seed = 0\n",
    "\n",
    "det_weights_path = model_upper_dir + f'model_seed_{seed}/best_weights.pth'\n",
    "det_model.load_state_dict(torch.load(det_weights_path, map_location=device))\n",
    "\n",
    "for p in det_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "det_model.eval()"
   ],
   "id": "487989f7334e3a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generator and Discriminator",
   "id": "eece651bb90d4e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dim = 4      # Z dimension\n",
    "gen_hidden = 3\n",
    "gen_nodes = 20\n",
    "\n",
    "class StochasticSubMap(nn.Module):\n",
    "    def __init__(self, d_of_x=1, noise_dim=4, n_hidden=3, n_nodes=20):\n",
    "        super().__init__()\n",
    "        in_dim  = d_of_x + noise_dim\n",
    "        out_dim = d_of_x\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dim, n_nodes))\n",
    "        for _ in range(n_hidden - 1):\n",
    "            layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.last_linear   = nn.Linear(n_nodes, out_dim)\n",
    "        self.activation    = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # x: (B, d_of_x), z: (B, noise_dim)\n",
    "        inp = torch.cat([x, z], dim=1)          # (B, d_of_x + noise_dim)\n",
    "        residual   = inp\n",
    "        for layer in self.hidden_layers:\n",
    "            residual = self.activation(layer(residual))\n",
    "        residual = self.last_linear(residual)              # (B, d_of_x)\n",
    "        return residual\n"
   ],
   "id": "6872640a4e848b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "critic_hidden = 3\n",
    "critic_nodes  = 20\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, d_of_x=1, L=40, n_hidden=3, n_nodes=64):\n",
    "        super().__init__()\n",
    "        in_dim = d_of_x + L * d_of_x\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dim, n_nodes))\n",
    "        for _ in range(n_hidden - 1):\n",
    "            layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.last_linear   = nn.Linear(n_nodes, 1)\n",
    "        self.activation    = nn.ReLU()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # seq: (B, 1 + L)\n",
    "        out = seq\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.activation(layer(out))\n",
    "        out = self.last_linear(out)              # (B, 1)\n",
    "        return out\n"
   ],
   "id": "bb6f1ea95ee5aa83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "G_stoch = StochasticSubMap(d_of_x=d_of_x, noise_dim=noise_dim, n_hidden=gen_hidden, n_nodes=gen_nodes).to(device)\n",
    "C_critic = Critic(d_of_x=d_of_x, L=L, n_hidden=critic_hidden, n_nodes=critic_nodes).to(device)\n",
    "\n",
    "lambda_gp = 10.0\n",
    "n_critic  = 5     # n_ct\n",
    "n_epochs_gan = n_epochs # 100_000\n",
    "\n",
    "opt_C = optim.Adam(C_critic.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "opt_G = optim.Adam(G_stoch.parameters(), lr=5e-5, betas=(0.5, 0.999))\n"
   ],
   "id": "378e053e02764b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_fake_sequence(det_model, G_stoch, x0_batch, L, noise_dim):\n",
    "    B = x0_batch.size(0)\n",
    "    x_b = x0_batch  # (B, 1)\n",
    "    y_list = []\n",
    "\n",
    "    for j in range(L):\n",
    "        z = torch.randn(B, noise_dim, device=x0_batch.device)\n",
    "\n",
    "        with torch.no_grad():           # fix det_model\n",
    "            det_next = det_model(x_b)   # (B, 1)\n",
    "\n",
    "        det_inc  = det_next - x_b       # D(x) - x\n",
    "        sto_inc  = G_stoch(x_b, z)      # S(x, z)\n",
    "        y_b      = det_inc + sto_inc    # total increment\n",
    "        x_b      = x_b + y_b            # update state\n",
    "\n",
    "        y_list.append(y_b)\n",
    "\n",
    "    y_fake = torch.cat(y_list, dim=1)          # (B, L)\n",
    "    seq_fake = torch.cat([x0_batch, y_fake], dim=1)  # (B, L+1)\n",
    "    return seq_fake\n"
   ],
   "id": "491933c9c50650fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_save_gan_checkpoints = n_epochs_gan/100\n",
    "step = 0\n",
    "for epoch in range(n_epochs_gan):\n",
    "    for (real_batch,) in gan_loader:        # each element is (real_sequences,)\n",
    "        real_batch = real_batch.to(device)  # (B, 1+L)\n",
    "        B = real_batch.size(0)\n",
    "\n",
    "        # Split into x0 and y1 till yL\n",
    "        x0_batch = real_batch[:, :d_of_x]   # (B,1)\n",
    "\n",
    "        # Update Critic\n",
    "        opt_C.zero_grad()\n",
    "\n",
    "        # Generate fake sequences with current generator\n",
    "        fake_batch = generate_fake_sequence(det_model, G_stoch, x0_batch, L, noise_dim)\n",
    "\n",
    "        # Critic scores\n",
    "        D_real = C_critic(real_batch)      # (B,1)\n",
    "        D_fake = C_critic(fake_batch)      # (B,1)\n",
    "\n",
    "        # Gradient penalty: sample epslon in [0,1]\n",
    "        eps = torch.rand(B, 1, device=device)\n",
    "        eps = eps.expand_as(real_batch)    # (B, 1+L)\n",
    "\n",
    "        x_hat = eps * real_batch + (1 - eps) * fake_batch\n",
    "        x_hat.requires_grad_(True)\n",
    "\n",
    "        D_hat = C_critic(x_hat)\n",
    "\n",
    "        grad_outputs = torch.ones_like(D_hat)\n",
    "        gradients = autograd.grad(\n",
    "            outputs=D_hat,\n",
    "            inputs=x_hat,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]                               # (B, 1+L)\n",
    "\n",
    "        grad_norm = gradients.view(B, -1).norm(2, dim=1)   # gradient\n",
    "        gp = ((grad_norm - 1.0) ** 2).mean()               # gradient penalty term\n",
    "\n",
    "        # WGAN critic loss\n",
    "        loss_C = D_fake.mean() - D_real.mean() + lambda_gp * gp\n",
    "        loss_C.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        # Update Generator every n_critic steps\n",
    "        if step % n_critic == 0:\n",
    "            opt_G.zero_grad()\n",
    "\n",
    "            fake_batch = generate_fake_sequence(det_model, G_stoch, x0_batch, L, noise_dim)\n",
    "            D_fake_for_G = C_critic(fake_batch)\n",
    "\n",
    "            # maximize critic's output = minimize -E[D(fake)]\n",
    "            loss_G = -D_fake_for_G.mean()\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # epoch log\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        if step % n_critic == 0:\n",
    "           print(f\"[Epoch {epoch+1}] C_loss: {loss_C.item():.4f}, G_loss: {loss_G.item():.4f}\")\n",
    "        else:\n",
    "           print(f\"[Epoch {epoch+1}] C_loss: {loss_C.item():.4f}\")\n",
    "\n",
    "    if (epoch + 1) % n_save_gan_checkpoints == 0:\n",
    "        save_folder = os.path.join(model_upper_dir, f\"{n_epochs_gan}gan_epochs/checkpoints\")\n",
    "        save_path = os.path.join(save_folder, f\"gan_checkpoint_epoch_{epoch+1}.pth\")\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        torch.save({\n",
    "            'generator': G_stoch.state_dict(),\n",
    "            'critic': C_critic.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, save_path)"
   ],
   "id": "40da4e598a7b9e43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_folder = os.path.join(model_upper_dir, f\"{n_epochs_gan}gan_epochs\")\n",
    "save_path = os.path.join(save_folder, \"gan_stochastic_submap.pth\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "torch.save(G_stoch.state_dict(), save_path)"
   ],
   "id": "c1d68d71c2bd8c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sFML_step(det_model, G_stoch, x, noise_dim, device):\n",
    "    B = x.size(0)\n",
    "    z = torch.randn(B, noise_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        det_next = det_model(x)\n",
    "    det_inc = det_next - x\n",
    "    sto_inc = G_stoch(x, z)\n",
    "    y = det_inc + sto_inc\n",
    "    return x + y\n"
   ],
   "id": "218e2797f14b3399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_stochastic_trajectory(det_model, G_stoch, x0, n_steps, noise_dim, device):\n",
    "    det_model.eval()\n",
    "    G_stoch.eval()\n",
    "\n",
    "    # Make x a batch of size 1, shape (1, d_of_x)\n",
    "    if isinstance(x0, (float, int)):\n",
    "        x = torch.tensor([[x0]], dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        x0_arr = np.array(x0, dtype=np.float32).reshape(1, -1)\n",
    "        x = torch.from_numpy(x0_arr).to(device)\n",
    "\n",
    "    states = [x.detach().cpu().numpy().copy()]  # store x_0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            B = x.size(0)\n",
    "\n",
    "            # Sample noise\n",
    "            z = torch.randn(B, noise_dim, device=device)\n",
    "\n",
    "            det_next = det_model(x)                 # (B, d_of_x)\n",
    "            det_inc  = det_next - x                 # D_delta(x) - x\n",
    "            sto_inc  = G_stoch(x, z)                # (B, d_of_x)\n",
    "            y = det_inc + sto_inc                   # (B, d_of_x)\n",
    "            x = x + y                               # (B, d_of_x)\n",
    "\n",
    "            states.append(x.detach().cpu().numpy().copy())\n",
    "\n",
    "    trajectory = np.concatenate(states, axis=0)          # (n_steps+1, d_of_x)\n",
    "    t = np.arange(n_steps + 1)\n",
    "\n",
    "    return t, trajectory\n"
   ],
   "id": "17cf0c2e8daa8ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_steps   = N_rec\n",
    "noise_dim = noise_dim\n",
    "x0_value  = 0.5\n",
    "\n",
    "t, traj = sample_stochastic_trajectory(det_model, G_stoch,\n",
    "                                       x0=x0_value,\n",
    "                                       n_steps=n_steps,\n",
    "                                       noise_dim=noise_dim,\n",
    "                                       device=device)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(t, traj[:, 0])\n",
    "plt.xlabel(\"Step index n\")\n",
    "plt.ylabel(\"x_n\")\n",
    "plt.title(\"Sample stochastic trajectory from sFML\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "4ff7cbdd16d809f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x0_value = 1\n",
    "runs = 100\n",
    "n, t = 100, 1\n",
    "nt = n*t\n",
    "fake_generation = np.zeros((nt + 1, runs))\n",
    "plot_dir = model_upper_dir\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(runs):\n",
    "    t, generation = sample_stochastic_trajectory(det_model, G_stoch,\n",
    "                                           x0=x0_value,\n",
    "                                           n_steps=n,\n",
    "                                           noise_dim=noise_dim,\n",
    "                                           device=device)\n",
    "    fake_generation[:, i] = generation[:, 0]\n",
    "\n",
    "plt.plot(t_grid, fake_generation[:, :15], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Step index n\")\n",
    "plt.ylabel(\"x_n\")\n",
    "plt.title(\"Multiple stochastic trajectories\")\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f\"{plot_dir}/stochastic_trajectories.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.savefig(f\"./stochastic_trajectories.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "e1c890c747f15891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fake_mean = fake_generation.mean(axis=1)\n",
    "fake_std = fake_generation.std(axis=1)\n",
    "\n",
    "true_mean = S.mean(axis=1)\n",
    "true_std  = S.std(axis=1)\n",
    "\n",
    "plt.plot(t_grid, true_mean, color=\"blue\", label=\"Ground Truth Mean\", linewidth=2)\n",
    "plt.plot(t_grid, fake_mean, color=\"red\", label=\"Fake Mean\", linewidth=2)\n",
    "plt.fill_between(t_grid[:, 0], true_mean - true_std, true_mean + true_std, alpha=0.25, color=\"lightblue\", label=\"Ground Truth Std\")\n",
    "plt.fill_between(t_grid[:, 0], fake_mean - fake_std, fake_mean + fake_std, alpha=0.25, color=\"green\", label=\"Fake Std\")\n",
    "\n",
    "plt.xlabel(\"T\")\n",
    "plt.ylabel(\"X_t\")\n",
    "plt.title(\"Mean and Standard Deviation of Geometric Brownian Motion\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "# plt.savefig(f\"predictionvsactualstats.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "1f8bff0f7cd574cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finding the best model combination",
   "id": "38c9b9825ff39ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 0\n",
    "modeldet_weights_path = model_upper_dir + f'model_seed_{seed}/best_weights.pth'\n",
    "print(modeldet_weights_path)\n",
    "\n",
    "modelgan_weights_path = model_upper_dir + f'100000gan_epochs/checkpoints/'\n",
    "print(modelgan_weights_path)\n",
    "# for j in range(1_000, 101_000, 1_000):\n",
    "for ep in [79]:\n",
    "    # if j < 5000:\n",
    "        j = ep*1000\n",
    "        currentgan_path = modelgan_weights_path + \"gan_checkpoint_epoch_\" + str(j) + \".pth\"\n",
    "        print(currentgan_path)\n",
    "        checkpoint = torch.load(currentgan_path, map_location=device)\n",
    "        G_stoch.load_state_dict(checkpoint['generator'])\n",
    "\n",
    "        x0_value = 1\n",
    "        runs = 100\n",
    "        n, t = 100, 1\n",
    "        nt = n*t\n",
    "        fake_generation = np.zeros((nt + 1, runs))\n",
    "        plot_dir = model_upper_dir\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for i in range(runs):\n",
    "            t, generation = sample_stochastic_trajectory(det_model, G_stoch,\n",
    "                                                   x0=x0_value,\n",
    "                                                   n_steps=n,\n",
    "                                                   noise_dim=noise_dim,\n",
    "                                                   device=device)\n",
    "            fake_generation[:, i] = generation[:, 0]\n",
    "\n",
    "\n",
    "        fake_mean = fake_generation.mean(axis=1)\n",
    "        fake_std = fake_generation.std(axis=1)\n",
    "\n",
    "        plt.plot(t_grid, true_mean, color=\"blue\", label=\"Ground Truth Mean\", linewidth=2)\n",
    "        plt.plot(t_grid, fake_mean, color=\"red\", label=\"Fake Mean\", linewidth=2)\n",
    "        plt.fill_between(t_grid[:, 0], true_mean - true_std, true_mean + true_std, alpha=0.25, color=\"lightblue\", label=\"Ground Truth Std\")\n",
    "        plt.fill_between(t_grid[:, 0], fake_mean - fake_std, fake_mean + fake_std, alpha=0.25, color=\"green\", label=\"Fake Std\")\n",
    "\n",
    "        plt.xlabel(\"T\")\n",
    "        plt.ylabel(\"X_t\")\n",
    "        plt.title(f\"Mean and Standard Deviation of GBM {j}\")\n",
    "        plt.legend()\n",
    "        plt.grid(False)\n",
    "        # plt.savefig(f\"predictionvsactualstats.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ],
   "id": "c0f898c00f41ba6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Drift Diffusion plot",
   "id": "2cbed5e5a144d011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mu = 2\n",
    "x_axis = np.linspace(0, 16, 100)\n",
    "true_a = x_axis * mu\n",
    "delta = 0.01\n",
    "\n",
    "a_hat = []\n",
    "for x_i in x_axis:\n",
    "    current = torch.tensor(x_i, device=device, dtype=torch.float32)\n",
    "    x0_batch = current.expand(10000, 1)\n",
    "    x1_batch = sFML_step(det_model, G_stoch, x0_batch, noise_dim=noise_dim, device=device)\n",
    "\n",
    "    increase = x1_batch - x0_batch\n",
    "    drift = (increase.mean()/delta).item()\n",
    "    # drift = increase.mean().items()\n",
    "    a_hat.append(drift)\n",
    "\n",
    "a_hat = np.array(a_hat)\n",
    "\n",
    "plt.plot(x_axis, true_a, color=\"blue\", label=\"Reference\", linewidth=2)\n",
    "plt.plot(x_axis, a_hat, color=\"red\", label=\"Predicted\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"b(x)\")\n",
    "plt.title(\"drift term a(x) = mu * x\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ],
   "id": "93ce94d5df06a81e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sigma = 1\n",
    "delta = 0.01\n",
    "\n",
    "b_hat = []\n",
    "\n",
    "for x_i in x_axis:\n",
    "    # batch of 5000 samples of x_i\n",
    "    current = torch.tensor(x_i, device=device, dtype=torch.float32)\n",
    "    x0_batch = current.expand(5000, 1)\n",
    "\n",
    "    # one stochastic step\n",
    "    x1_batch = sFML_step(det_model, G_stoch, x0_batch,\n",
    "                         noise_dim=noise_dim, device=device)\n",
    "\n",
    "    # increments\n",
    "    increase = x1_batch - x0_batch    # (5000, 1)\n",
    "\n",
    "    # standard deviation of increments\n",
    "    std_inc = increase.std().item()   # torch â†’ float\n",
    "\n",
    "    # diffusion estimator b(x) = std_inc / sqrt(delta)\n",
    "    b_val = std_inc / np.sqrt(delta)\n",
    "\n",
    "    b_hat.append(b_val)\n",
    "\n",
    "b_hat = np.array(b_hat)\n",
    "\n",
    "# true diffusion\n",
    "true_b = x_axis * sigma\n",
    "\n",
    "plt.plot(x_axis, true_b,  color=\"blue\",  label=\"Reference\", linewidth=2)\n",
    "plt.plot(x_axis, b_hat,   color=\"red\",   label=\"Predicted\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"b(x)\")\n",
    "plt.title(\"diffusion term b(x) = sigma * x\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ],
   "id": "b7f346acd74bdb99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1fea366fcbf7d067",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
